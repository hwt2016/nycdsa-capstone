tree.carseats
#The output shows the variables used at each node, the split rule, the number
#of observations at each node, the deviance based on the Gini index, the
#majority class value based on the observations in the node, and the associated
#probabilities of class membership at each node. Terminal nodes are denoted
#by asterisks.
#Splitting the data into training and test sets by an 70% - 30% split.
set.seed(0)
train = sample(1:nrow(Carseats), 7*nrow(Carseats)/10) #Training indices.
Carseats.test = Carseats[-train, ] #Test dataset.
High.test = High[-train] #Test response.
#Ftting and visualizing a classification tree to the training data.
tree.carseats = tree(High ~ . - Sales, data = Carseats, subset = train)
plot(tree.carseats)
text(tree.carseats, pretty = 0)
summary(tree.carseats)
tree.carseats
#Using the trained decision tree to classify the test data.
tree.pred = predict(tree.carseats, Carseats.test, type = "class")
tree.pred
#Assessing the accuracy of the overall tree by constructing a confusion matrix.
table(tree.pred, High.test)
(60 + 42)/120
table(tree.pred, High.test)
table(tree.pred,Purchase.Test)
library(tree)
library(ISLR)
attach(OJ)
help(OJ)
names(OJ)
#split the data by 80%-20%
set.seed(0)
train = sample(1:nrow(OJ), 8*nrow(OJ)/10) #Training indices.
OJ.test = OJ[-train, ] #Test dataset.
OJ.train = OJ[train,]
Purchase.Test = Purchase[-train]
#construct initial tree
tree.train = tree(Purchase ~ ., split = "gini", data = OJ.train)
summary(tree.train)
#Number of terminal nodes:  86
#Misclassification error rate: 0.1449 = 124 / 856
plot(tree.train)
text(tree.train, pretty = 0)
#predict the test set
tree.pred = predict(tree.train, OJ.test, type = "class")
tree.pred
summary(tree.pred)
table(tree.pred,Purchase.Test)
165/(106+24+25+59)
set.seed(0)
cv.OJ = cv.tree(tree.train, FUN = prune.misclass)
names(cv.OJ)
cv.OJ
par(mfrow = c(1, 1))
plot(cv.OJ$size, cv.OJ$dev, type = "b",
xlab = "Terminal Nodes", ylab = "Misclassified Observations")
plot(cv.OJ$k, cv.OJ$dev, type  = "b",
xlab = "Alpha", ylab = "Misclassified Observations")
par(mfrow = c(1,2))
prune.OJ = prune.misclass(tree.train, best = 10)
plot(prune.OJ)
text(prune.OJ, pretty = 0)
prune.OJ = prune.misclass(tree.train, best = 40)
plot(prune.OJ)
text(prune.OJ, pretty = 0)
prune.pred = predict(prune.OJ, OJ.test, type = "class")
table(prune.pred,Purchase.Test)
prune.pred = predict(prune.OJ, OJ.test, type = "class")
table(prune.pred,Purchase.Test)
(101+59)/(101+24+30+59)
library(randomForest)
set.seed(0)
rf.OJ = randomForest(Purchase~ ., data = OJ, subset = train, importance = TRUE)
rf.OJ
(447+239)/(95+75+447+239)
rf.OJ_fr = randomForest(Purchase~ ., data = OJ, subset = train, importance = TRUE)
rf.OJ_fr
set.seed(0)
rf.OJ_fr = randomForest(Purchase~ ., data = OJ, subset = train, importance = TRUE)
rf.OJ_fr
pred.forest = predict(rf.OJ_fr, OJ.test, type = 'class')
pred.forest
table(pred.forest, OJ.test)
table(pred.forest, Purchase.Test)
(113+60)/(113+23+18+60)
OJ.train.indicator = OJ.train
OJ.test.indicator = OJ.test
OJ.train.indicator$Purchase = as.vector(OJ.train$Purchase, mode =
"numeric") - 1
OJ.test.indicator$Purchase = as.vector(OJ.test$Purchase, mode =
"numeric") - 1
set.seed(0)
library(gbm)
install.packages("gbm")
?gbm
?gbm()
library(gbm)
?gbm()
boost.OJ = gbm(Purchase ~ ., data = OJ[train, ],
distribution = "bernoulli",
n.trees = 1000,
interaction.depth = 4,
shrinkage = 0.001)
set.seed(0)
oob.err = numeric(17)
for (mtry in 1:17) {
fit = randomForest(Purchase ~ ., data = OJ[train, ], mtry = mtry)
oob.err[mtry] = fit$mse[500]
cat("We're performing iteration", mtry, "\n")
}
fit = randomForest(Purchase ~ ., data = OJ[train, ], mtry = mtry)
fit
summary(fit)
oob.err[mtry] = fit$err.rate[500]
cat("We're performing iteration", mtry, "\n")
set.seed(0)
oob.err = numeric(17)
for (mtry in 1:17) {
fit = randomForest(Purchase ~ ., data = OJ[train, ], mtry = mtry)
oob.err[mtry] = fit$err.rate[500]
cat("We're performing iteration", mtry, "\n")
}
plot(1:17, oob.err, pch = 16, type = "b",
xlab = "Variables Considered at Each Split",
ylab = "OOB Mean Squared Error",
main = "Random Forest OOB Error Rates\nby # of Variables")
par(mfrow=c(1,1))
plot(1:17, oob.err, pch = 16, type = "b",
xlab = "Variables Considered at Each Split",
ylab = "OOB Mean Squared Error",
main = "Random Forest OOB Error Rates\nby # of Variables")
bag.rf.OJ=randomForest(Purchase ~., data=OJ.train, mtry=17, ntree=500,
importance=TRUE)
bag.rf.OJ
(437+245)/(437+85+89+245)
set.seed(0)
rf.pred.bag=predict(bag.rf.OJ, OJ.test, type='class')
table(rf.pred.bag, Purchase.test)
table(rf.pred.bag, Purchase.Test)
(109+62)/(109+21+22+62)
OJ.train.indicator$Purchase = as.vector(OJ.train$Purchase, mode =
"numeric") - 1
OJ.train.indicator$Purchase
boost.OJ = gbm(J.train.indicator$Purchase ~ ., data = OJ[train, ],
distribution = "bernoulli",
n.trees = 1000,
interaction.depth = 4,
shrinkage = 0.001)
boost.OJ = gbm(OJ.train.indicator$Purchase ~ ., data = OJ[train, ],
distribution = "bernoulli",
n.trees = 1000,
interaction.depth = 4,
shrinkage = 0.001)
boost.OJ
?predict
library(gbm)
predmat2 = predict(boost.OJ, newdata = OJ.test.indicator, n.trees = 10000, type = 'response')
n.trees = seq(100,10000,100)
n.trees
predmat2 = predict(boost.OJ, newdata = OJ.test.indicator, n.trees = n.trees, type = 'response')
predmat2
berr = with(OJ.test.indicator, apply((predmat2 - medv)^2, 2, mean))
berr = with(OJ.test.indicator, apply((predmat2 - Purchase)^2, 2, mean))
plot(n.trees, berr, pch = 16,
ylab = "Mean Squared Error",
xlab = "# Trees",
main = "Boosting Test Error")
abline(y=min(berr),col='red')
?abline
abline(a=min(berr),col='red')
abline(h=min(berr),col='red')
min(berr
)
abline(h=1-min(berr),col='blue')
predmat2 = predict(boost.OJ, newdata = OJ.test.indicator, n.trees = n.trees, type = 'response')
summary(predmat2)
(447+239)/(95+75+447+239)
(113+60)/(113+23+18+60)
names(Boston)
help(Boston)
library(MASS)
help(Boston)
bag.rf.OJ=randomForest(Purchase ~., data=OJ.train, mtry=17, ntree=500,
importance=TRUE)
bag.rf.OJ
install.packages("kernlab")
library(kernlab)
data(spam)
attach(spam)
?spam
head(spam)
nrow(spam)
fa.parallel(spam[,-"type"], #The data in question.
n.obs = 4601, #Since we supplied a covaraince matrix, need to know n.
fa = "pc", #Display the eigenvalues for PCA.
n.iter = 100)
library(psych)
install.packages("psych")
library(psych)
fa.parallel(spam[,-"type"], #The data in question.
n.obs = 4601, #Since we supplied a covaraince matrix, need to know n.
fa = "pc", #Display the eigenvalues for PCA.
n.iter = 100)
spam[-type]
spam[,-type]
spam[,.-type]
spam[,`~-type]
spam[,~-type]
fa.parallel(spam[-58], #The data in question.
n.obs = 4601, #Since we supplied a covaraince matrix, need to know n.
fa = "pc", #Display the eigenvalues for PCA.
n.iter = 100) #Number of simulated analyses to perform.
abline(h = 1)
pc_spam = principal(spam2, #The data in question.
nfactors = 4, #The number of PCs to extract.
rotate = "none")
pc_spam
class(names(spam))
names(spam)
pc_spam = principal(spam2, #The data in question.
nfactors = 4, #The number of PCs to extract.
rotate = "none")
pc_spam
spam2 = spam[-58]
spam2 = spam[-58]
fa.parallel(spam2, #The data in question.
n.obs = 4601, #Since we supplied a covaraince matrix, need to know n.
fa = "pc", #Display the eigenvalues for PCA.
n.iter = 100) #Number of simulated analyses to perform.
abline(h = 1)
pc_spam = principal(spam2, #The data in question.
nfactors = 4, #The number of PCs to extract.
rotate = "none")
pc_spam
factor.plot(pc_spam,
labels = colnames(spam2))
?spam
library(psych)
bodies = Harman23.cor$cov #Covariance matrix of 8 physical measurements on 305 girls.
bodies
fa.parallel(bodies, #The data in question.
n.obs = 305, #Since we supplied a covaraince matrix, need to know n.
fa = "pc", #Display the eigenvalues for PCA.
n.iter = 100) #Number of simulated analyses to perform.
abline(h = 1)
?fa.parallel
iris_meas = iris[, -5] #Measurements of iris dataset.
iris_meas
plot(iris_meas)
plot(spam2)
plot(spam2)
par(mfrow=c(1,1))
plot(spam2)
library(Sleuth2)
case1701
printer_data = case1701[, 1:11]
fa.parallel(printer_data, #The data in question.
fa = "pc", #Display the eigenvalues for PCA.
n.iter = 100) #Number of simulated analyses to perform.
abline(h = 1) #Adding a horizontal line at 1.
#Should extract 1 PC, but let's look at 3.
pc_printer = principal(printer_data, #The data in question.
nfactors = 3,
rotate = "none") #The number of PCs to extract.
pc_printer
install.packages("Sleuth2")
library(Sleuth2)
case1701
printer_data = case1701[, 1:11]
fa.parallel(printer_data, #The data in question.
fa = "pc", #Display the eigenvalues for PCA.
n.iter = 100) #Number of simulated analyses to perform.
abline(h = 1) #Adding a horizontal line at 1.
#Should extract 1 PC, but let's look at 3.
pc_printer = principal(printer_data, #The data in question.
nfactors = 3,
rotate = "none") #The number of PCs to extract.
pc_printer
factor.plot(pc_printer) #Add variable names to the plot.
plot(printer_data)
pairs(pc_printer$scores)
?principal
library(Sleuth2)
case1701
printer_data = case1701[, 1:11]
fa.parallel(printer_data, #The data in question.
fa = "pc", #Display the eigenvalues for PCA.
n.iter = 100) #Number of simulated analyses to perform.
abline(h = 1) #A
fa.parallel(spam2, #The data in question.
n.obs = 4601, #Since we supplied a covaraince matrix, need to know n.
fa = "pc", #Display the eigenvalues for PCA.
n.iter = 100) #Number of simulated analyses to perform.
abline(h = 1)
summary(pc_spam)
pc_spam2 = principal(spam2, #The data in question.
nfactors = 1, #The number of PCs to extract.
rotate = "none")
pc_spam2
factor.plot(pc_spam2,
labels = colnames(spam2))
pc_spam = principal(spam2, #The data in question.
nfactors = 4, #The number of PCs to extract.
rotate = "none")
pc_spam
factor.plot(pc_spam,
labels = colnames(spam2))
summary(pc_spam)
library(dplyr)
new = data %>% filter(MonthlyIncome == 0)%>%group_by(NumberOfDependents) %>% summarise(mean(MonthlyIncome))
setwd('F:/Miaozhi/Academic/Data_Science/Bootcamp/Project_Capstone/nycdsa-capstone')
data = read.csv('./data/cs-training-outlier-f10.csv', header =T)
names(data)
library(dplyr)
new = data %>% filter(data$MonthlyIncome == 0)%>%group_by(data$NumberOfDependents) %>% summarise(mean(data$MonthlyIncome))
new = data %>% filter(data$MonthlyIncome == 0)%>%group_by(data$NumberOfDependents) %>% summarise(mean(data$MonthlyIncome,na.rm=T))
new = data %>% filter(MonthlyIncome != 0)%>%group_by(NumberOfDependents) %>% summarise(mean(MonthlyIncome,na.rm=T))
View(new)
setwd('F:/Miaozhi/Academic/Data_Science/Bootcamp/Project_Capstone/nycdsa-capstone')
data = read.csv('./data/cs-training-outlier-f10.csv', header =T)
names(data)
library(dplyr)
new = data %>% filter(MonthlyIncome != 0)%>%group_by(NumberOfDependents) %>% summarise(mean(MonthlyIncome,na.rm=T))
DepdRisk = 1:150000
data = read.csv('./data/cs-training-outlier-f10.csv', header =T)
setwd('F:/Miaozhi/Academic/Data_Science/Bootcamp/Project_Capstone/nycdsa-capstone')
data = read.csv('./data/cs-training-outlier-f10.csv', header =T)
library(dplyr)
new = data %>% filter(MonthlyIncome != 0)%>%group_by(NumberOfDependents) %>% summarise(mean(MonthlyIncome,na.rm=T))
View(new)
names(new)
new = data %>% filter(MonthlyIncome != 0)%>%group_by(NumberOfDependents) %>% summarise(avg=mean(MonthlyIncome,na.rm=T))
names(new)
new[NumberOfDependents==data$NumberOfDependents[6]]
new[new$NumberOfDependents==data$NumberOfDependents[6]]$avg
new[new$NumberOfDependents==data$NumberOfDependents[i],]$avg
new[new$NumberOfDependents==data$NumberOfDependents[1],]$avg
setwd('F:/Miaozhi/Academic/Data_Science/Bootcamp/Project_Capstone/nycdsa-capstone')
data = read.csv('./data/cs-training-outlier-f10.csv', header =T)
names(data)
library(dplyr)
new = data %>% filter(MonthlyIncome != 0)%>%group_by(NumberOfDependents) %>% summarise(avg=mean(MonthlyIncome,na.rm=T))
names(new)
DepdRisk = 1:150000
for(i in 1:150000){
if(is.na(data$MonthlyIncome[i])){
DepdRisk[i] = NA
}else{
if(data$MonthlyIncome[i] == 0){
DepdRisk[i] = data$NumberOfDependents/(new[new$NumberOfDependents==data$NumberOfDependents[i],]$avg)
}else{
DepdRisk[i] = data$NumberOfDependents/as.numeric(data$MonthlyIncome)
}
}
}
which(data$age == 0)
which(data$MonthlyIncome == 0)
DepdRisk[149951]
data$NumberOfDependents[149951]
setwd('F:/Miaozhi/Academic/Data_Science/Bootcamp/Project_Capstone/nycdsa-capstone')
data = read.csv('./data/cs-training-outlier-f10.csv', header =T)
library(dplyr)
new = data %>% filter(MonthlyIncome != 0)%>%group_by(NumberOfDependents) %>% summarise(avg=median(MonthlyIncome,na.rm=T))
DepdRisk = 1:150000
DepdRisk[1:1000]
for(i in 1:150000){
if(is.na(data$MonthlyIncome[i])){
DepdRisk[i] = NA
}else{
if(data$MonthlyIncome[i] == 0){
DepdRisk[i] = data$NumberOfDependents/(new[new$NumberOfDependents==data$NumberOfDependents[i],]$avg)
}else{
DepdRisk[i] = data$NumberOfDependents/as.numeric(data$MonthlyIncome)
}
}
}
DepdRisk[1:1000]
data$MonthlyIncome[1:1000]
data$MonthlyIncome[919]
data$NumberOfDependents[919]
2/5259
DepdRisk = 1:150000
for(i in 1:150000){
if(is.na(data$MonthlyIncome[i])){
DepdRisk[i] = NA
}else{
if(data$MonthlyIncome[i] == 0){
DepdRisk[i] = data$NumberOfDependents[i]/(new[new$NumberOfDependents==data$NumberOfDependents[i],]$avg)
}else{
DepdRisk[i] = data$NumberOfDependents[i]/as.numeric(data$MonthlyIncome[i])
}
}
}
data = as.data.frame(cbind(data, DepdRisk))
View(data)
data = data[,-c(7,12)]
write.csv(data,'cs-training-depdrisk-f09.csv')
test = read.csv('./data/cs-test-outlier-f10.csv', header = T)
DepdRisk[919]
setwd('F:/Miaozhi/Academic/Data_Science/Bootcamp/Project_Capstone/nycdsa-capstone')
test = read.csv('./data/cs-test-outlier-f10.csv', header = T)
new = test %>% filter(MonthlyIncome != 0)%>%group_by(NumberOfDependents) %>% summarise(avg=median(MonthlyIncome,na.rm=T))
names(new)
DepdRisk = 1:101503
for(i in 1:101503){
if(is.na(test$MonthlyIncome[i])){
DepdRisk[i] = NA
}else{
if(test$MonthlyIncome[i] == 0){
DepdRisk[i] = test$NumberOfDependents[i]/(new[new$NumberOfDependents==test$NumberOfDependents[i],]$avg)
}else{
DepdRisk[i] = test$NumberOfDependents[i]/as.numeric(test$MonthlyIncome[i])
}
}
}
test = as.data.frame(cbind(test, DepdRisk))
test = test[,-c(7,12)]
write.csv(data,'cs-test-depdrisk-f09.csv')
setwd('F:/Miaozhi/Academic/Data_Science/Bootcamp/Project_Capstone/nycdsa-capstone')
data = read.csv('./data/cs-training-outlier-f10.csv', header =T)
AgeRisk=1:150000
for(i in 1:150000){
if(data$age[i]<29){
AgeRisk[i]= 1-637/850
}else{
if(data$age[i]<39){
AgeRisk[i]=1-654/850
}else{
if(data$age[i]<49){
AgeRisk[i]=1-675/850
}else{
if(data$age[i]<59){
AgeRisk[i]=1-697/850
}else{
if(data$age[i]<69){
AgeRisk[i]=1-722/850
}else{
AgeRisk[i]=1-747/850
}
}
}
}
}
}
r=c(0.105,0.089,0.119)
sum = sum(r)
w1 = r[1]/sum
w2 = r[2]/sum
w3 = r[3]/sum
default_time = w1 * data$NumberOfTime3059DaysPastDueNotWorse +w2 * data$NumberOfTime6089DaysPastDueNotWorse + w3 * data$NumberOfTimes90DaysLate
DebtAmt = data$DebtRatio * as.numeric(data$MonthlyIncome)
new = data %>% filter(MonthlyIncome != 0)%>%group_by(NumberOfDependents) %>% summarise(avg=median(MonthlyIncome,na.rm=T))
DepdRisk = 1:150000
for(i in 1:150000){
if(is.na(data$MonthlyIncome[i])){
DepdRisk[i] = NA
}else{
if(data$MonthlyIncome[i] == 0){
DepdRisk[i] = data$NumberOfDependents[i]/(new[new$NumberOfDependents==data$NumberOfDependents[i],]$avg)
}else{
DepdRisk[i] = data$NumberOfDependents[i]/as.numeric(data$MonthlyIncome[i])
}
}
}
data = as.data.frame(cbind(data,AgeRisk,default_time,DebtAmt,DepdRisk))
data = data[,-c(4,5,6,7,9,11,12)]
write.csv(data,'cs-training-combine-f07.csv')
test = read.csv('./data/cs-test-outlier-f10.csv')
AgeRisk=1:101503
for(i in 1:101503){
if(test$age[i]<29){
AgeRisk[i]= 1-637/850
}else{
if(test$age[i]<39){
AgeRisk[i]=1-654/850
}else{
if(test$age[i]<49){
AgeRisk[i]=1-675/850
}else{
if(test$age[i]<59){
AgeRisk[i]=1-697/850
}else{
if(test$age[i]<69){
AgeRisk[i]=1-722/850
}else{
AgeRisk[i]=1-747/850
}
}
}
}
}
}
default_time = w1 * test$NumberOfTime3059DaysPastDueNotWorse +w2 * test$NumberOfTime6089DaysPastDueNotWorse + w3 * test$NumberOfTimes90DaysLate
DebtAmt = test$DebtRatio * as.numeric(test$MonthlyIncome)
new = test %>% filter(MonthlyIncome != 0)%>%group_by(NumberOfDependents) %>% summarise(avg=median(MonthlyIncome,na.rm=T))
DepdRisk = 1:101503
for(i in 1:101503){
if(is.na(test$MonthlyIncome[i])){
DepdRisk[i] = NA
}else{
if(test$MonthlyIncome[i] == 0){
DepdRisk[i] = test$NumberOfDependents[i]/(new[new$NumberOfDependents==test$NumberOfDependents[i],]$avg)
}else{
DepdRisk[i] = test$NumberOfDependents[i]/as.numeric(test$MonthlyIncome[i])
}
}
}
test = as.data.frame(cbind(test,AgeRisk,default_time,DebtAmt,DepdRisk))
test = test[,-c(4,5,7,9,11,12)]
write.csv(test,'cs-test-combine-f07.csv')
DeptRisk[1:1000]
DepdRisk[1:1000]
