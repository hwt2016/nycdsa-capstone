names(OJ)
set.seed(0)
train = sample(1:nrow(OJ), 8*nrow(OJ)/10) #Training indices.
OJ.test = OJ[-train, ] #Test dataset.
OJ.train = OJ[train,]
tree.train = tree(Purchase ~ ., split = "gini", data = OJ.train)
summary(tree.train)
plot(tree.train)
text(tree.train, pretty = 0)
class(Purchase)
tree.pred = predict(tree.train, OJ.test, type = "class")
tree.pred
summary(tree.pred)
Purchase.Test = Purchase[-train]
table(tree.pred,Purchase.Test)
106/(106+24)
cv.OJ = cv.tree(tree.train, FUN = prune.misclass)
names(cv.OJ)
cv.OJ
names(cv.OJ)
par(mfrow = c(1, 2))
plot(cv.OJ$size, cv.OJ$dev, type = "b",
xlab = "Terminal Nodes", ylab = "Misclassified Observations")
plot(cv.OJ$k, cv.OJ$dev, type  = "b",
xlab = "Alpha", ylab = "Misclassified Observations")
par(mfrow = c(1, 2))
plot(cv.OJ$size, cv.OJ$dev, type = "b",
xlab = "Terminal Nodes", ylab = "Misclassified Observations")
par(mfrow = c(1, 1))
plot(cv.OJ$size, cv.OJ$dev, type = "b",
xlab = "Terminal Nodes", ylab = "Misclassified Observations")
plot(cv.OJ$k, cv.OJ$dev, type  = "b",
xlab = "Alpha", ylab = "Misclassified Observations")
plot(cv.OJ$size, cv.OJ$dev, type = "b",
plot(cv.OJ$size, cv.OJ$dev, type = "b",
xlab = "Terminal Nodes", ylab = "Misclassified Observations")
par(mfrow = c(1, 1))
plot(cv.OJ$size, cv.OJ$dev, type = "b",
xlab = "Terminal Nodes", ylab = "Misclassified Observations")
par(mfrow = c(1,2))
prune.OJ = prune.misclass(tree.OJ, best = 10)
plot(prune.OJ)
text(prune.OJ, pretty = 0)
prune.OJ = prune.misclass(tree.OJ, best = 40)
plot(prune.OJ)
text(prune.OJ, pretty = 0)
prune.OJ = prune.misclass(tree.train, best = 10)
plot(prune.OJ)
text(prune.OJ, pretty = 0)
prune.OJ = prune.misclass(tree.train, best = 40)
plot(prune.OJ)
text(prune.OJ, pretty = 0)
par(mfrow = c(1,2))
prune.OJ = prune.misclass(tree.train, best = 10)
plot(prune.OJ)
text(prune.OJ, pretty = 0)
prune.OJ = prune.misclass(tree.train, best = 40)
plot(prune.OJ)
text(prune.OJ, pretty = 0)
prune.OJ = prune.misclass(tree.train, best = 40)
summary(prune.OJ)
prune.OJ = prune.misclass(tree.train, best = 10)
summary(prune.OJ)
prune.pred = predict(prune.OJ, OJ.test, type = "class")
table(prune.pred,Purchase.Test)
112/(112+25)
106/(106+24)
library(randomForest)
install.packages("randomForest")
library(randomForest)
rf.OJ = randomForest(Purchase~ ., data = OJ, subset = train, importance = TRUE)
rf.OJ
rf.OJ = randomForest(Purchase~ ., data = OJ, subset = OJ.test, importance = TRUE)
rf.OJ = randomForest(Purchase~ ., data = OJ.test, importance = TRUE)
rf.OJ
importance(rf.OJ)
names(OJ)
set.seed(0)
oob.err = numeric(18)
for (mtry in 1:18) {
fit = randomForest(medv ~ ., data = OJ[train, ], mtry = mtry)
oob.err[mtry] = fit$mse[500]
cat("We're performing iteration", mtry, "\n")
}
set.seed(0)
oob.err = numeric(18)
for (mtry in 1:18) {
fit = randomForest(Purchase ~ ., data = OJ[train, ], mtry = mtry)
oob.err[mtry] = fit$mse[500]
cat("We're performing iteration", mtry, "\n")
}
oob.err = numeric(18)
oob.err
library(tree)
#Loading the ISLR library in order to use the Carseats dataset.
library(ISLR)
#Making data manipulation easier.
help(Carseats)
attach(Carseats)
#Looking at the variable of interest, Sales.
hist(Sales)
summary(Sales)
#Creating a binary categorical variable High based on the continuous Sales
#variable and adding it to the original data frame.
High = ifelse(Sales <= 8, "No", "Yes")
Carseats = data.frame(Carseats, High)
#Fit a tree to the data; note that we are excluding Sales from the formula.
tree.carseats = tree(High ~ . - Sales, split = "gini", data = Carseats)
summary(tree.carseats)
#The output shows the variables actually used within the tree, the number of
#terminal nodes, the residual mean deviance based on the Gini index, and
#the misclassification error rate.
#Plotting the classification tree.
plot(tree.carseats)
text(tree.carseats, pretty = 0) #Yields category names instead of dummy variables.
#Detailed information for the splits of the classification tree.
tree.carseats
#The output shows the variables used at each node, the split rule, the number
#of observations at each node, the deviance based on the Gini index, the
#majority class value based on the observations in the node, and the associated
#probabilities of class membership at each node. Terminal nodes are denoted
#by asterisks.
#Splitting the data into training and test sets by an 70% - 30% split.
set.seed(0)
train = sample(1:nrow(Carseats), 7*nrow(Carseats)/10) #Training indices.
Carseats.test = Carseats[-train, ] #Test dataset.
High.test = High[-train] #Test response.
#Ftting and visualizing a classification tree to the training data.
tree.carseats = tree(High ~ . - Sales, data = Carseats, subset = train)
plot(tree.carseats)
text(tree.carseats, pretty = 0)
summary(tree.carseats)
tree.carseats
#Using the trained decision tree to classify the test data.
tree.pred = predict(tree.carseats, Carseats.test, type = "class")
tree.pred
#Assessing the accuracy of the overall tree by constructing a confusion matrix.
table(tree.pred, High.test)
(60 + 42)/120
table(tree.pred, High.test)
table(tree.pred,Purchase.Test)
library(tree)
library(ISLR)
attach(OJ)
help(OJ)
names(OJ)
#split the data by 80%-20%
set.seed(0)
train = sample(1:nrow(OJ), 8*nrow(OJ)/10) #Training indices.
OJ.test = OJ[-train, ] #Test dataset.
OJ.train = OJ[train,]
Purchase.Test = Purchase[-train]
#construct initial tree
tree.train = tree(Purchase ~ ., split = "gini", data = OJ.train)
summary(tree.train)
#Number of terminal nodes:  86
#Misclassification error rate: 0.1449 = 124 / 856
plot(tree.train)
text(tree.train, pretty = 0)
#predict the test set
tree.pred = predict(tree.train, OJ.test, type = "class")
tree.pred
summary(tree.pred)
table(tree.pred,Purchase.Test)
165/(106+24+25+59)
set.seed(0)
cv.OJ = cv.tree(tree.train, FUN = prune.misclass)
names(cv.OJ)
cv.OJ
par(mfrow = c(1, 1))
plot(cv.OJ$size, cv.OJ$dev, type = "b",
xlab = "Terminal Nodes", ylab = "Misclassified Observations")
plot(cv.OJ$k, cv.OJ$dev, type  = "b",
xlab = "Alpha", ylab = "Misclassified Observations")
par(mfrow = c(1,2))
prune.OJ = prune.misclass(tree.train, best = 10)
plot(prune.OJ)
text(prune.OJ, pretty = 0)
prune.OJ = prune.misclass(tree.train, best = 40)
plot(prune.OJ)
text(prune.OJ, pretty = 0)
prune.pred = predict(prune.OJ, OJ.test, type = "class")
table(prune.pred,Purchase.Test)
prune.pred = predict(prune.OJ, OJ.test, type = "class")
table(prune.pred,Purchase.Test)
(101+59)/(101+24+30+59)
library(randomForest)
set.seed(0)
rf.OJ = randomForest(Purchase~ ., data = OJ, subset = train, importance = TRUE)
rf.OJ
(447+239)/(95+75+447+239)
rf.OJ_fr = randomForest(Purchase~ ., data = OJ, subset = train, importance = TRUE)
rf.OJ_fr
set.seed(0)
rf.OJ_fr = randomForest(Purchase~ ., data = OJ, subset = train, importance = TRUE)
rf.OJ_fr
pred.forest = predict(rf.OJ_fr, OJ.test, type = 'class')
pred.forest
table(pred.forest, OJ.test)
table(pred.forest, Purchase.Test)
(113+60)/(113+23+18+60)
OJ.train.indicator = OJ.train
OJ.test.indicator = OJ.test
OJ.train.indicator$Purchase = as.vector(OJ.train$Purchase, mode =
"numeric") - 1
OJ.test.indicator$Purchase = as.vector(OJ.test$Purchase, mode =
"numeric") - 1
set.seed(0)
library(gbm)
install.packages("gbm")
?gbm
?gbm()
library(gbm)
?gbm()
boost.OJ = gbm(Purchase ~ ., data = OJ[train, ],
distribution = "bernoulli",
n.trees = 1000,
interaction.depth = 4,
shrinkage = 0.001)
set.seed(0)
oob.err = numeric(17)
for (mtry in 1:17) {
fit = randomForest(Purchase ~ ., data = OJ[train, ], mtry = mtry)
oob.err[mtry] = fit$mse[500]
cat("We're performing iteration", mtry, "\n")
}
fit = randomForest(Purchase ~ ., data = OJ[train, ], mtry = mtry)
fit
summary(fit)
oob.err[mtry] = fit$err.rate[500]
cat("We're performing iteration", mtry, "\n")
set.seed(0)
oob.err = numeric(17)
for (mtry in 1:17) {
fit = randomForest(Purchase ~ ., data = OJ[train, ], mtry = mtry)
oob.err[mtry] = fit$err.rate[500]
cat("We're performing iteration", mtry, "\n")
}
plot(1:17, oob.err, pch = 16, type = "b",
xlab = "Variables Considered at Each Split",
ylab = "OOB Mean Squared Error",
main = "Random Forest OOB Error Rates\nby # of Variables")
par(mfrow=c(1,1))
plot(1:17, oob.err, pch = 16, type = "b",
xlab = "Variables Considered at Each Split",
ylab = "OOB Mean Squared Error",
main = "Random Forest OOB Error Rates\nby # of Variables")
bag.rf.OJ=randomForest(Purchase ~., data=OJ.train, mtry=17, ntree=500,
importance=TRUE)
bag.rf.OJ
(437+245)/(437+85+89+245)
set.seed(0)
rf.pred.bag=predict(bag.rf.OJ, OJ.test, type='class')
table(rf.pred.bag, Purchase.test)
table(rf.pred.bag, Purchase.Test)
(109+62)/(109+21+22+62)
OJ.train.indicator$Purchase = as.vector(OJ.train$Purchase, mode =
"numeric") - 1
OJ.train.indicator$Purchase
boost.OJ = gbm(J.train.indicator$Purchase ~ ., data = OJ[train, ],
distribution = "bernoulli",
n.trees = 1000,
interaction.depth = 4,
shrinkage = 0.001)
boost.OJ = gbm(OJ.train.indicator$Purchase ~ ., data = OJ[train, ],
distribution = "bernoulli",
n.trees = 1000,
interaction.depth = 4,
shrinkage = 0.001)
boost.OJ
?predict
library(gbm)
predmat2 = predict(boost.OJ, newdata = OJ.test.indicator, n.trees = 10000, type = 'response')
n.trees = seq(100,10000,100)
n.trees
predmat2 = predict(boost.OJ, newdata = OJ.test.indicator, n.trees = n.trees, type = 'response')
predmat2
berr = with(OJ.test.indicator, apply((predmat2 - medv)^2, 2, mean))
berr = with(OJ.test.indicator, apply((predmat2 - Purchase)^2, 2, mean))
plot(n.trees, berr, pch = 16,
ylab = "Mean Squared Error",
xlab = "# Trees",
main = "Boosting Test Error")
abline(y=min(berr),col='red')
?abline
abline(a=min(berr),col='red')
abline(h=min(berr),col='red')
min(berr
)
abline(h=1-min(berr),col='blue')
predmat2 = predict(boost.OJ, newdata = OJ.test.indicator, n.trees = n.trees, type = 'response')
summary(predmat2)
(447+239)/(95+75+447+239)
(113+60)/(113+23+18+60)
names(Boston)
help(Boston)
library(MASS)
help(Boston)
bag.rf.OJ=randomForest(Purchase ~., data=OJ.train, mtry=17, ntree=500,
importance=TRUE)
bag.rf.OJ
install.packages("kernlab")
library(kernlab)
data(spam)
attach(spam)
?spam
head(spam)
nrow(spam)
fa.parallel(spam[,-"type"], #The data in question.
n.obs = 4601, #Since we supplied a covaraince matrix, need to know n.
fa = "pc", #Display the eigenvalues for PCA.
n.iter = 100)
library(psych)
install.packages("psych")
library(psych)
fa.parallel(spam[,-"type"], #The data in question.
n.obs = 4601, #Since we supplied a covaraince matrix, need to know n.
fa = "pc", #Display the eigenvalues for PCA.
n.iter = 100)
spam[-type]
spam[,-type]
spam[,.-type]
spam[,`~-type]
spam[,~-type]
fa.parallel(spam[-58], #The data in question.
n.obs = 4601, #Since we supplied a covaraince matrix, need to know n.
fa = "pc", #Display the eigenvalues for PCA.
n.iter = 100) #Number of simulated analyses to perform.
abline(h = 1)
pc_spam = principal(spam2, #The data in question.
nfactors = 4, #The number of PCs to extract.
rotate = "none")
pc_spam
class(names(spam))
names(spam)
pc_spam = principal(spam2, #The data in question.
nfactors = 4, #The number of PCs to extract.
rotate = "none")
pc_spam
spam2 = spam[-58]
spam2 = spam[-58]
fa.parallel(spam2, #The data in question.
n.obs = 4601, #Since we supplied a covaraince matrix, need to know n.
fa = "pc", #Display the eigenvalues for PCA.
n.iter = 100) #Number of simulated analyses to perform.
abline(h = 1)
pc_spam = principal(spam2, #The data in question.
nfactors = 4, #The number of PCs to extract.
rotate = "none")
pc_spam
factor.plot(pc_spam,
labels = colnames(spam2))
?spam
library(psych)
bodies = Harman23.cor$cov #Covariance matrix of 8 physical measurements on 305 girls.
bodies
fa.parallel(bodies, #The data in question.
n.obs = 305, #Since we supplied a covaraince matrix, need to know n.
fa = "pc", #Display the eigenvalues for PCA.
n.iter = 100) #Number of simulated analyses to perform.
abline(h = 1)
?fa.parallel
iris_meas = iris[, -5] #Measurements of iris dataset.
iris_meas
plot(iris_meas)
plot(spam2)
plot(spam2)
par(mfrow=c(1,1))
plot(spam2)
library(Sleuth2)
case1701
printer_data = case1701[, 1:11]
fa.parallel(printer_data, #The data in question.
fa = "pc", #Display the eigenvalues for PCA.
n.iter = 100) #Number of simulated analyses to perform.
abline(h = 1) #Adding a horizontal line at 1.
#Should extract 1 PC, but let's look at 3.
pc_printer = principal(printer_data, #The data in question.
nfactors = 3,
rotate = "none") #The number of PCs to extract.
pc_printer
install.packages("Sleuth2")
library(Sleuth2)
case1701
printer_data = case1701[, 1:11]
fa.parallel(printer_data, #The data in question.
fa = "pc", #Display the eigenvalues for PCA.
n.iter = 100) #Number of simulated analyses to perform.
abline(h = 1) #Adding a horizontal line at 1.
#Should extract 1 PC, but let's look at 3.
pc_printer = principal(printer_data, #The data in question.
nfactors = 3,
rotate = "none") #The number of PCs to extract.
pc_printer
factor.plot(pc_printer) #Add variable names to the plot.
plot(printer_data)
pairs(pc_printer$scores)
?principal
library(Sleuth2)
case1701
printer_data = case1701[, 1:11]
fa.parallel(printer_data, #The data in question.
fa = "pc", #Display the eigenvalues for PCA.
n.iter = 100) #Number of simulated analyses to perform.
abline(h = 1) #A
fa.parallel(spam2, #The data in question.
n.obs = 4601, #Since we supplied a covaraince matrix, need to know n.
fa = "pc", #Display the eigenvalues for PCA.
n.iter = 100) #Number of simulated analyses to perform.
abline(h = 1)
summary(pc_spam)
pc_spam2 = principal(spam2, #The data in question.
nfactors = 1, #The number of PCs to extract.
rotate = "none")
pc_spam2
factor.plot(pc_spam2,
labels = colnames(spam2))
pc_spam = principal(spam2, #The data in question.
nfactors = 4, #The number of PCs to extract.
rotate = "none")
pc_spam
factor.plot(pc_spam,
labels = colnames(spam2))
summary(pc_spam)
setwd('F:/Miaozhi/Academic/Data_Science/Bootcamp/Project_Capstone/nycdsa-capstone')
data = read.csv('./data/cs-test-log-f10.csv', header =T)
View(data)
data = read.csv('./data/cs-training-log-f10.csv', header =T)
View(data)
data = data[ ,-c(1,2)]
View(data)
library(VIM)
library(mice)
library(adabag)
library(corrplot)
library(caret)
library(neuralnet)
library(nnet)
library(devtools)
library(pROC)
library(clusterGeneration)
library(NeuralNetTools)
library(mice)
library(dplyr)
library(ggplot2)
library(RColorBrewer)
library(psych)
library(ggpubr)
library(reshape2)
summary(data)
corrplot(cor(data[complete.cases(data),]), method="circle", type = "lower", order="hclust",
col=colorRampPalette(brewer.pal(11,"Spectral"))(8))
cor(data[complete.cases(data),])
corrplot(cor(data[complete.cases(data),]), method="circle", type = "lower", order="hclust",
col=colorRampPalette(brewer.pal(11,"Spectral"))(8))
qplot(x=Var1, y=Var2, data=melt(cor(data[complete.cases(data),], use="p")), fill=value, geom="tile") +
scale_fill_gradient2(limits=c(-1, 1))
corrplot(cor(data[complete.cases(data),]), method="circle", type = "lower", order="hclust",
col=colorRampPalette(brewer.pal(11,"Spectral"))(8))
data = read.csv('./data/cs-training-log-f10.csv', header =T')
data = read.csv('./data/cs-training-log-f10.csv', header =T)
data = data[,-c(1,2)]
Debt = data$MonthlyIncome * data$DebtRatio
data = as.data.frame(cbind(data,Debt))
View(data)
setwd('F:/Miaozhi/Academic/Data_Science/Bootcamp/Project_Capstone/nycdsa-capstone')
data = read.csv('./data/cs-training-log-f10.csv', header =T)
data = data[,-1]
Debt = data$MonthlyIncome * data$DebtRatio
data = as.data.frame(cbind(data,Debt))
data = data[,-c(6,7)]
write.csv(data, './data/cs-training-log-combine-f09.csv')
test = read.csv('./data/cs-test-log-f10.csv', header =T)
View(test)
test = data[,-1]
Debt = test$MonthlyIncome * test$DebtRatio
test = read.csv('./data/cs-test-log-f10.csv', header =T)
test = data[,-1]
test = read.csv('./data/cs-test-log-f10.csv', header =T)
test = test[,-1]
Debt = test$MonthlyIncome * test$DebtRatio
test = test[,-c(6,7)]
write.csv(data, './data/cs-test-log-combine-f09.csv')
setwd('F:/Miaozhi/Academic/Data_Science/Bootcamp/Project_Capstone/nycdsa-capstone')
data = read.csv('./data/cs-training.csv', header =T)
Detb = data$MonthlyIncome * data$DebtRatio
Debt = data$MonthlyIncome * data$DebtRatio
data = as.data.frame(cbind(data,Debt))
summary(data)
data = data[,-c(6,7)]
data$RevolvingUtilizationOfUnsecuredLines = log(data$RevolvingUtilizationOfUnsecuredLines+1)
data$NumberOfTime30.59DaysPastDueNotWorse = log(data$NumberOfTime30.59DaysPastDueNotWorse+1)
data$Debt = log(data$Debt+1)
data$NumberOfOpenCreditLinesAndLoans = log(data$NumberOfOpenCreditLinesAndLoans+1)
data$NumberOfTimes90DaysLate = log(data$NumberOfTimes90DaysLate+1)
data$NumberOfTime60.89DaysPastDueNotWorse = log(data$NumberOfTime60.89DaysPastDueNotWorse+1)
data$NumberOfDependents = log(data$NumberOfDependents+1)
data$NumberRealEstateLoansOrLines = log(data$NumberRealEstateLoansOrLines+1)
write.csv(data,'./data/cs-training-combine-log-f09.csv')
test = read.csv('./data/cs-test.csv', header =T)
Debt = test$MonthlyIncome * test$DebtRatio
test = as.data.frame(cbind(test,Debt))
test = test[,-c(6,7)]
test$RevolvingUtilizationOfUnsecuredLines = log(test$RevolvingUtilizationOfUnsecuredLines+1)
test$NumberOfTime30.59DaysPastDueNotWorse = log(test$NumberOfTime30.59DaysPastDueNotWorse+1)
test$Debt = log(test$Debt+1)
test$NumberOfOpenCreditLinesAndLoans = log(test$NumberOfOpenCreditLinesAndLoans+1)
test$NumberOfTimes90DaysLate = log(test$NumberOfTimes90DaysLate+1)
test$NumberOfTime60.89DaysPastDueNotWorse = log(test$NumberOfTime60.89DaysPastDueNotWorse+1)
test$NumberOfDependents = log(test$NumberOfDependents+1)
test$NumberRealEstateLoansOrLines = log(test$NumberRealEstateLoansOrLines+1)
write.csv(data,'./data/cs-test-combine-log-f09.csv')
